# 第一章 大型语言模型 LLM 介绍
## 1.1 大型语言模型（LLM）简介
**大语言模型（LLM，Large Language Model），也称大型语言模型，是一种旨在理解和生成人类语言的人工智能模型**。
LLM 通常指包含**数百亿（或更多）参数的语言模型**，它们在海量的文本数据上进行训练，从而获得对语言深层次的理解。目前，国外的知名 LLM 有 GPT、LLaMA、Gemini、Claude 和 Grok 等，国内的有 DeepSeek、通义千问、豆包、Kimi、文心一言、GLM 等。
通常大模型由三个阶段构成：预训练、后训练和在线推理。
### 1.1.1 常见的 LLM 模型
#### OpenAI 模型介绍
不断迭代，ChatGPT 逐渐丰富了其功能：

- 插件系统：允许开发者创建工具扩展 ChatGPT 的能力，实现网页浏览、数据分析和第三方服务调用
    
- 实时语音和视频对话：用户可与 AI 进行自然的语音和视频交流，支持手势识别和情感表达
    
- 多模态能力：能够分析和理解用户提供的图片、音频和视频，实现全面的多模态交互
    
- 自定义指令与记忆功能：记住用户之前的交互习惯和偏好，提供个性化体验
    
- GPT 构建器平台：允许用户无需编程创建专用的 AI 助手，支持自定义知识库和行为模式
    
- 数据分析与可视化：直接处理和分析上传的数据文件，生成图表和可视化报告
    
- 知识型与推理型双模式：可在 GPT-4.5 (知识型) 和 o1/o3 (推理型) 之间切换，满足不同场景需求
    
- 思维链展示：在推理型模型中可选择性展示思考过程，帮助用户理解推理步骤

- `2024 年 5 月` 发布的 **GPT-4o**（"o"代表"omni"全能）具备对文本、语音、图像三种模态的深度理解能力，主要特点包括：
    
    - **多模态融合**：无缝理解和生成多种形式内容
    - **实时对话**：响应速度比 GPT-4 快约 2 倍
    - **情感表达**：在语音互动中传递更丰富的情感变化
    - **成本效益**：API 定价降低约 50%
    - 
- `2024 年 9 月` 发布的 **o1-mini**、**o1-preview** 是专为复杂推理设计的模型，在回答前会先生成一段思维链（不公开），优先考虑精确性和推理步骤的正确性。
    - **超强推理能力**：在数学、编程和逻辑推理等任务中表现卓越
    - **解题过程可靠**：注重解题中间步骤的正确性
    - **问题分解能力**：将复杂问题分解为可管理的子问题
    - **自纠错机制**：识别错误并主动纠正

#### Claude 使用地址
由 OpenAI 离职人员创建的 **Anthropic** 公司开发的闭源语言大模型。
- 最早的 **Claude** 于 `2023 年 3 月 15 日` 发布。
- - `2025 年 2 月`，Anthropic 又进一步发布了 **Claude 3.7 Sonnet (Preview)**，这是目前是**首款混合推理模型**，支持标准模式与推理思考模式，**编码能力异常强大**。

#### [Gemini 使用地址](https://gemini.google.com/)

**PaLM 系列**语言大模型由 **Google** 开发。

- `2022 年 4 月`，发布了初始版本（PaLM 后更名为 Gemini）。
- `2025 年 2 月`，Google 发布了 **Gemini 2.0** 系列模型，在性能和效率上有显著提升。包括 Gemini 2.0 Pro、Gemini 2.0 Flash、Gemini 2.0 Flash-Lite 是 Gemini 2.0 系列的三个版本，分别适用于不同的场景。同样，推出了其推理模型 **Gemini 2.0 Flash Thinking**。
#### [文心一言使用地址](https://yiyan.baidu.com/)
**文心一言是基于百度文心大模型的知识增强语言大模型**，于 `2023 年 3 月` 在国内率先开启邀测。文心一言的基础模型文心大模型于 2019 年发布 1.0 版，现已更新到 **4.0** 版本。更进一步划分，文心大模型包括 NLP 大模型、CV 大模型、跨模态大模型、生物计算大模型、行业大模型。文心一言的中文能力相对来说非常不错。

文心一言网页版分为**免费版**和**专业版**。

- 免费版使用文心 3.5 版本，已经能够满足个人用户或小型企业的大部分需求。
- 专业版使用文心 4.0 版本，定价为 59.9 元/月，连续包月优惠价为 49.9 元/月。

同时也可以使用 API 进行调用
#### [星火大模型使用地址](https://xinghuo.xfyun.cn/)
**讯飞星火认知大模型**是**科大讯飞**发布的语言大模型，支持多种自然语言处理任务。

- `2023 年 5 月`，首次发布。
    
- `2024年 10 月`，讯飞星火发布模型 **星火 4.0 Turbo**。
    
- `2025 年 1 月`，讯飞发布了推理思考模型**讯飞星火 X1** 和 **星火语音同传模型**。
####  [LLaMA 官方地址](https://llama.meta.com/)

#### [LLaMA 开源地址](https://github.com/facebookresearch/llama)

**LaMA 系列模型**是 **Meta** 开源的一组参数规模 **从 8B 到 405B** 的基础语言模型。

- `2023 年 2 月`，发布 LLaMA。
- `2023 年 7 月`，发布了 LLaMA2 模型。
- `2024 年 4 月`，发布了 LLaMA3 模型。
- `2024 年 7 月`，发布了 **LLaMA 3.1** 模型。
- `2024 年 12 月`，发布了 **LLaMA 3.3** 模型(只开源了 70B 的指令模型)。

它们都是在数万亿个字符上训练的，展示了如何**仅使用公开可用的数据集来训练最先进的模型**，而不需要依赖专有或不可访问的数据集。这些数据集包括 Common Crawl、Wikipedia、OpenWebText2、RealNews、Books 等。LLaMA 模型使用了**大规模的数据过滤和清洗技术**，以提高数据质量和多样性，减少噪声和偏见。LLaMA 模型还使用了高效的**数据并行**和**流水线并行**技术，以加速模型的训练和扩展其中 405B 参数模型是首个公开的千亿级开源模型，性能对标 GPT-4o 等商业闭源模型。

与 GPT 系列相同，LLaMA 模型也采用了 **decoder-only** 架构，同时结合了一些前人工作的改进。LLaMA 系列基本上是后续大模型的标杆：

- `Pre-normalization 正则化`：为了提高训练稳定性，LLaMA 对每个 Transformer 子层的输入进行了 RMSNorm 归一化，这种归一化方法可以避免梯度爆炸和消失的问题，提高模型的收敛速度和性能；
- `SwiGLU 激活函数`：将 ReLU 非线性替换为 SwiGLU 激活函数，增加网络的表达能力和非线性，同时减少参数量和计算量；
- `旋转位置编码（RoPE，Rotary Position Embedding）`：模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码，RoPE 位置编码可以有效地捕捉输入序列中的相对位置信息，并且具有更好的泛化能力。
- `分组查询注意力（GQA，Grouped-Query Attention）`：通过将查询（query）分组并在组内共享键（key）和值（value），减少了计算量，同时保持了模型性能，提高了大型模型的推理效率。

**LLaMA 3.1** 于 `2024 年 7 月` 发布，提高了模型的性能和效率：

- `更多的训练数据量`：LLaMA3.1 在 15 万亿个 token 的数据上进行预训练，采用了更科学的数据配比。LLaMA3.1 接触到更多的文本信息，从而提高了其理解和生成文本的能力。
- `更长的上下文长度`：LLaMA 3.1 将上下文长度大幅提升至 128K token，支持处理极长的文档和对话历史，改善了对长文本的理解和生成能力，适用于更复杂的应用场景。
- `更多的训练数据量`：LLaMA3.1 在 15 万亿个 token 的数据上进行预训练，采用了更科学的数据配比。LLaMA3.1 接触到更多的文本信息，从而提高了其理解和生成文本的能力。
- `更长的上下文长度`：LLaMA 3.1 将上下文长度大幅提升至 128K token，支持处理极长的文档和对话历史，改善了对长文本的理解和生成能力，适用于更复杂的应用场景。
- `分组查询注意力（GQA，Grouped-Query Attention）`：通过将查询（query）分组并在组内共享键（key）和值（value），减少了计算量，同时保持了模型性能，提高了大型模型的推理效率（LLaMA2 只有 70B 采用）。
- `更大的词表`：LLaMA3.1 采用了 128K 的 tokenizer，是前两代 32K 的 4 倍，这使得其语义编码能力得到了极大的增强，从而显著提升了模型的性能。
- `精细的指令遵循`：通过改进的对齐技术，LLaMA 3.1 在遵循复杂指令、理解微妙提示方面表现更出色，使模型行为更可预测和可控。
- `完善的工具使用`：增强了 Function Calling 能力，使模型能够更准确地识别何时以及如何调用外部工具，提高了与外部系统集成的能力。

LLaMA 3.1 发布了 8B、70B 和 405B 三个规模的模型，分别提供基础版（Base）和指令微调版（Instruct），进一步扩展了 LLaMA 系列在开源社区的影响力和应用前景

####  [DeepSeek 使用地址](https://www.deepseek.com/)

####  [DeepSeek 开源地址](https://github.com/deepseek-ai)

**DeepSeek** 是由 **深度求索 (DeepSeek) 团队** 开发的开源大语言模型系列。首个版本于 `2023 年 11 月` 发布。DeepSeek 采用 **decoder-only** 架构，融合了 FlashAttention-2、RoPE 位置编码、SwiGLU 等先进技术，在多语言理解和代码生成等方面表现出色。

模型发展历程：

- `2023 年 11 月 12 日`：发布 DeepSeek 系列基础模型，包括 **7B 和 67B** 两种规模的 **Base** 和 **Chat** 版本。模型在 1.2 万亿 token 上进行训练，同时发布了 **DeepSeek-Coder** 专用代码生成模型。
- `2024 年 3 月 15 日`：发布 **DeepSeek-V2** 系列，提升了多语言能力、长文本理解和推理能力，同时发布了 **DeepSeek-MoE** 混合专家模型。
- `2024 年 5 月 31 日`：发布 **DeepSeek-V2.5**，性能得到进一步提升，上下文长度扩展至 **128K tokens**，并改进了工具调用和多模态能力。
- `2024 年 10 月`：发布 **[DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3)**，在推理能力、多语言理解和创意生成方面有显著提升，支持更复杂的系统提示词控制，并进一步提升了代码质量和多轮对话一致性。
- `2025 年 2 月`：
    - **[DeepSeekR1](https://github.com/deepseek-ai)** **推理型大模型**，专注于复杂问题求解和精确推理能力，在数学、逻辑推理和结构化知识方面展现出卓越性能，类似于 OpenAI 的 o1 系列。并且是**首个开源的推理型大模型**，在多项基准测试中超越了 o1 系列。
    - **DeepSeek-R1-Zero** 直接在大规模强化学习 (RL) 训练的模型，无需 SFT，在推理方面就十分出色。
    - 同时开源了用 Llama 和 Qwen 从 DeepSeek-R1 中蒸馏出的六个 dense 模型。 其中 DeepSeek-R1-Distill-Qwen-32B 在各种基准测试中均优于 OpenAI-o1-mini。

deepseek 目前采用的主要改进如下：

- 多头潜在注意力 (MLA，Multi-head Latent Attention) ：通过将键值 (KV) 缓存显著压缩为潜在向量来保证高效推理的同时不降低效果。
- DeepSeekMoE，通过稀疏计算以经济的成本训练强大的模型。
- 一系列推理加速技术

借助着 DeepSeekR1 的卓越能力，DeekSeep 成为了现象级爆火应用。7 天 完成了 1 亿用户的增长，打破了 ChatGPT 的 2 个月的最快记录，成为**史上增长最快**的 AI 应用。

#### [通义千问使用地址](https://tongyi.aliyun.com/)

#### [通义千问开源地址](https://github.com/QwenLM)

**通义千问由阿里巴巴基于 "通义" 大模型研发**，于 `2023 年 4 月` 正式发布。

- `2023 年 9 月`，阿里云开源了 Qwen（通义千问）系列工作。
- `2024 年 6 月 6 日`，正式开源了 **Qwen2**。
- `2025 年 4 月 29 日`，发布了全新升级的 **Qwen3** 系列模型。

Qwen 系列均采用 **decoder-Only** 架构，并结合 `SwiGLU 激活`、`RoPE`、`GQA` 等技术。中文能力相对来说非常不错的开源模型。

目前，已经开源了 7 种模型大小：**0.6B, 1.7B, 4B, 8B, 14B, 32B** 的 Dense 模型和 **30B-A3B, 235B-A22B** 的 MoE 模型；8B 以下模型的上下文长度为 32k，8B 以上模型的上下文长度为 128k。Qwen3 进一步增强了模型性能，改进了推理能力和指令遵循能力，同时保持了低资源部署的高效性，使其在长文本理解和复杂任务处理方面具有更强的优势。**支持思考模式和非思考模式之间无缝切换**。覆盖 **119 种语言和方言**。强化了模型的代码能力，Agent 能力，以及对 **MCP 的支持**。

同时还开源了代码模型和数学模型：

- Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的 32B;
- Qwen2.5-Math: 1.5B, 7B, 以及 72B。

在推理大模型方面：于 2024 年 11 月 发布并开源了 QwQ-32B-Preview 模型， 仅用 32B 参数便在部分达到了 o1-mini 的推理水平。

并于 `2025 年 3 月`发布并开源了 **QwQ-32B**，其性能可与具备 671B 参数（37B 激活参数）的 DeepSeek-R1 媲美。

#### [ChatGLM 使用地址](https://chatglm.cn/)

#### [ChatGLM 开源地址](https://github.com/THUDM/GLM-4)
**GLM 系列模型**是 **清华大学和智谱 AI 等** 合作研发的语言大模型。

- `2023 年 3 月`，发布了 **ChatGLM**。
- `2024 年 1 月`，发布了 **GLM4**，并于 `2024 年 6 月` 正式开源。

**GLM-4-9B-Chat** 支持多轮对话的同时，还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 **128K** 上下文）等功能。

开源了 `对话模型` **GLM-4-9B-Chat**、`基础模型` **GLM-4-9B**、`长文本对话模型` **GLM-4-9B-Chat-1M**（支持 1M 上下文长度）、`多模态模型` **GLM-4V-9B** 等全面对标 OpenAI：

#### [百川使用地址](https://www.baichuan-ai.com/chat)

####  [百川开源地址](https://github.com/baichuan-inc)

**Baichuan** 是由 **百川智能** 开发的 **开源可商用** 的语言大模型，其基于 **Transformer 解码器架构（decoder-only）**。

- `2023 年 6 月 15 日`，发布了 **Baichuan-7B** 和 **Baichuan-13B**。百川同时开源了 **预训练** 和 **对齐** 模型，`预训练模型是面向开发者的“基座”`，而 `对齐模型则面向广大需要对话功能的普通用户`。
- **Baichuan2** 于 `2023 年 9 月 6 日` 推出，发布了 **7B、13B** 的 **Base** 和 **Chat** 版本，并提供了 Chat 版本的 **4bits 量化**。
- `2024 年 1 月 29 日`，发布了 **Baichuan 3**，但是 **目前还没有开源**。

## [1.1.2 LLM 的特点与能力](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_112-llm-%e7%9a%84%e7%89%b9%e7%82%b9%e4%b8%8e%e8%83%bd%e5%8a%9b)

1. **巨大的规模：** LLM 通常具有巨大的参数规模，可以达到数十亿甚至数千亿个参数。这使得它们能够捕捉更多的语言知识和复杂的语法结构。
    
2. **预训练和微调：** LLM 采用了预训练和微调的学习方法。首先在大规模文本数据上进行预训练（无标签数据），学习通用的语言表示和知识。然后通过微调（有标签数据）适应特定任务，从而在各种 NLP 任务中表现出色。
    
3. **上下文感知：** LLM 在处理文本时具有强大的上下文感知能力，能够理解和生成依赖于前文的文本内容。这使得它们在对话、文章生成和情境理解方面表现出色。
    
4. **多语言支持：** LLM 可以用于多种语言，不仅限于英语。它们的多语言能力使得跨文化和跨语言的应用变得更加容易。
    
5. **多模态支持：** 一些 LLM 已经扩展到支持多模态数据，包括文本、图像和声音。使得它们可以理解和生成不同媒体类型的内容，实现更多样化的应用。
    
6. **伦理和风险问题：** 尽管 LLM 具有出色的能力，但它们也引发了伦理和风险问题，包括生成有害内容、隐私问题、认知偏差等。因此，研究和应用 LLM 需要谨慎。
    
7. **高计算资源需求：** LLM 参数规模庞大，需要大量的计算资源进行训练和推理。通常需要使用高性能的 GPU 或 TPU 集群来实现。
    

大语言模型是一种具有强大语言处理能力的技术，已经在多个领域展示了潜力。它们为自然语言理解和生成任务提供了强大的工具，同时也引发了对其伦理和风险问题的关注。这些特点使 LLM 成为了当今计算机科学和人工智能领域的重要研究和应用方向。

#### [1.1.2.1 涌现能力（emergent abilities）](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_1121-%e6%b6%8c%e7%8e%b0%e8%83%bd%e5%8a%9b%ef%bc%88emergent-abilities%ef%bc%89)
#### [1.1.2.1 涌现能力（emergent abilities）](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_1121-%e6%b6%8c%e7%8e%b0%e8%83%bd%e5%8a%9b%ef%bc%88emergent-abilities%ef%bc%89)

区分大语言模型（LLM）与以前的预训练语言模型（PLM）最显著的特征之一是它们的 `涌现能力`。涌现能力是一种令人惊讶的能力，它在小型模型中不明显，但在大型模型中特别突出。类似物理学中的相变现象，涌现能力就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的 **量变引起质变**。

涌现能力可以与某些复杂任务有关，但我们更关注的是其通用能力。接下来，我们简要介绍三个 LLM 典型的涌现能力：

1. **上下文学习**：上下文学习能力是由 GPT-3 首次引入的。这种能力允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。
    
2. **指令遵循**：通过使用自然语言描述的多任务数据进行微调，也就是所谓的 `指令微调`。LLM 被证明在使用指令形式化描述的未见过的任务上表现良好。这意味着 LLM 能够根据任务指令执行任务，而无需事先见过具体示例，展示了其强大的泛化能力。
    
3. **逐步推理**：小型语言模型通常难以解决涉及多个推理步骤的复杂任务，例如数学问题。然而，LLM 通过采用 `思维链（CoT, Chain of Thought）` 推理策略，利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案。据推测，这种能力可能是通过对代码的训练获得的。
    

这些涌现能力让 LLM 在处理各种任务时表现出色，使它们成为了解决复杂问题和应用于多领域的强大工具。

#### [1.1.2.2 作为基座模型支持多元应用的能力](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_1122-%e4%bd%9c%e4%b8%ba%e5%9f%ba%e5%ba%a7%e6%a8%a1%e5%9e%8b%e6%94%af%e6%8c%81%e5%a4%9a%e5%85%83%e5%ba%94%e7%94%a8%e7%9a%84%e8%83%bd%e5%8a%9b)

在 2021 年，斯坦福大学等多所高校的研究人员提出了基座模型（foundation model）的概念，清晰了预训练模型的作用。这是一种全新的 AI 技术范式，借助于海量无标注数据的训练，获得可以适用于大量下游任务的大模型（单模态或者多模态）。这样，**多个应用可以只依赖于一个或少数几个大模型进行统一建设**。

大语言模型是这个新模式的典型例子，使用统一的大模型可以极大地提高研发效率。相比于每次开发单个模型的方式，这是一项本质上的进步。大型模型不仅可以缩短每个具体应用的开发周期，减少所需人力投入，也可以基于大模型的推理、常识和写作能力，获得更好的应用效果。因此，大模型可以成为 AI 应用开发的大一统基座模型，这是一个一举多得、全新的范式，值得大力推广。

#### [1.1.2.3 支持对话作为统一入口的能力](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_1123-%e6%94%af%e6%8c%81%e5%af%b9%e8%af%9d%e4%bd%9c%e4%b8%ba%e7%bb%9f%e4%b8%80%e5%85%a5%e5%8f%a3%e7%9a%84%e8%83%bd%e5%8a%9b)

让大语言模型真正火爆的契机，是基于对话聊天的 **ChatGPT**。业界很早就发现了用户对于对话交互的特殊偏好，陆奇在微软期间，就于 2016 年推进过“对话即平台（conversation as a platform）” 的战略。此外，苹果 Siri 、亚马逊 Echo 等基于语音对话的产品也非常受欢迎，反映出互联网用户对于聊天和对话这种交互模式的偏好。虽然之前的聊天机器人存在各种问题，但大型语言模型的出现再次让聊天机器人这种交互模式可以重新涌现。用户愈发期待像钢铁侠中“贾维斯”一样的人工智能，无所不能、无所不知。这引发我们对于 `智能体（Agent）` 类型应用前景的思考，Auto-GPT、微软 Jarvis 等项目已经出现并受到关注，相信未来会涌现出很多类似的以对话形态让助手完成各种具体工作的项目。

LLM 已经在许多领域产生了深远的影响。在**自然语言处理**领域，它可以帮助计算机更好地理解和生成文本，包括写文章、回答问题、翻译语言等。在**信息检索**领域，它可以改进搜索引擎，让我们更轻松地找到所需的信息。在**计算机视觉**领域，研究人员还在努力让计算机理解图像和文字，以改善多媒体交互。

最重要的是，LLM 的出现让人们重新思考了 **通用人工智能（AGI）** 的可能性。AGI 是一种像人类一样思考和学习的人工智能。LLM 被认为是 AGI 的一种早期形式，这引发了对未来人工智能发展的许多思考和计划。

总之，LLM 是一种令人兴奋的技术，它让计算机更好地理解和使用语言，正在改变着我们与技术互动的方式，同时也引发了对未来人工智能的无限探索。

> 在下一章我们将介绍 LLM 时期一个重要的技术 RAG。

【**参考内容**】：

1. [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)
2. [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)
3. [周枫：当我们谈论大模型时，应该关注哪些新能力？](https://xueqiu.com/1389978604/248392718)
4. [S 型智能增长曲线：从 Deepseek R1 看 Scaling Law 的未来](https://zhuanlan.zhihu.com/p/22658624635)
5. [一文详尽之 Scaling Law！](https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247700024&idx=1&sn=7933ecfaa8d0e127d70d671aff418545&chksm=e9ec360d1ae201b6f875055fe0f83d808076dcadc38144be050297213ff6d8085b241fdb4319&scene=0&xtrack=1)
6. [QwQ: 思忖未知之界](https://qwenlm.github.io/zh/blog/qwq-32b-preview/)
7. [QwQ-32B: 领略强化学习之力](https://qwenlm.github.io/zh/blog/qwq-32b/)
## [1.2 什么是 RAG](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_12-%e4%bb%80%e4%b9%88%e6%98%af-rag)

目前 LLM 面临的主要问题有：

- **信息偏差/幻觉：** LLM 有时会产生与客观事实不符的信息，导致用户接收到的信息不准确。RAG 通过检索数据源，辅助模型生成过程，确保输出内容的精确性和可信度，减少信息偏差。
    
- **知识更新滞后性：** LLM 基于静态的数据集训练，这可能导致模型的知识更新滞后，无法及时反映最新的信息动态。RAG 通过实时检索最新数据，保持内容的时效性，确保信息的持续更新和准确性。
    
- **内容不可追溯：** LLM 生成的内容往往缺乏明确的信息来源，影响内容的可信度。RAG 将生成内容与检索到的原始资料建立链接，增强了内容的可追溯性，从而提升了用户对生成内容的信任度。
    
- **领域专业知识能力欠缺：** LLM 在处理特定领域的专业知识时，效果可能不太理想，这可能会影响到其在相关领域的回答质量。RAG 通过检索特定领域的相关文档，为模型提供丰富的上下文信息，从而提升了在专业领域内的问题回答质量和深度。
    
- **推理能力限制：** 面对复杂问题时，LLM 可能缺乏必要的推理能力，这影响了其对问题的理解和回答。RAG 结合检索到的信息和模型的生成能力，通过提供额外的背景知识和数据支持，增强了模型的推理和理解能力。
    
- **应用场景适应性受限：** LLM 需在多样化的应用场景中保持高效和准确，但单一模型可能难以全面适应所有场景。RAG 使得 LLM 能够通过检索对应应用场景数据的方式，灵活适应问答系统、推荐系统等多种应用场景。
    
- **长文本处理能力较弱：** LLM 在理解和生成长篇内容时受限于有限的上下文窗口，且必须按顺序处理内容，输入越长，速度越慢。RAG 通过检索和整合长文本信息，强化了模型对长上下文的理解和生成，有效突破了输入长度的限制，同时降低了调用成本，并提升了整体的处理效率。
### [1.2.1 RAG 的工作流程](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_121-rag-%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b)

RAG 是一个完整的系统，其工作流程可以简单地分为数据处理、检索、增强和生成四个阶段：

1. **数据处理阶段**
    1. 对原始数据进行清洗和处理。
    2. 将处理后的数据转化为检索模型可以使用的格式。
    3. 将处理后的数据存储在对应的数据库中。
2. **检索阶段**
    1. 将用户的问题输入到检索系统中，从数据库中检索相关信息。
3. **增强阶段**
    1. 对检索到的信息进行处理和增强，以便生成模型可以更好地理解和使用。
4. **生成阶段**
    1. 将增强后的信息输入到生成模型中，生成模型根据这些信息生成答案。

### [1.2.2 RAG VS Finetune](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_122-rag-vs-finetune)

在提升大语言模型效果中，RAG 和 微调（Finetune）是两种主流的方法。

**微调**: 通过在特定数据集上进一步训练大语言模型，来提升模型在特定任务上的表现。

RAG 和 微调的对比可以参考下表（表格来源[[1](https://arxiv.org/abs/2312.10997)][[2](https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey)]）

|特征比较|RAG|微调|
|---|---|---|
|知识更新|直接更新检索知识库，无需重新训练。信息更新成本低，适合动态变化的数据。|通常需要重新训练来保持知识和数据的更新。更新成本高，适合静态数据。|
|外部知识|擅长利用外部资源，特别适合处理文档或其他结构化/非结构化数据库。|将外部知识学习到 LLM 内部。|
|数据处理|对数据的处理和操作要求极低。|依赖于构建高质量的数据集，有限的数据集可能无法显著提高性能。|
|模型定制|侧重于信息检索和融合外部知识，但可能无法充分定制模型行为或写作风格。|可以根据特定风格或术语调整 LLM 行为、写作风格或特定领域知识。|
|可解释性|可以追溯到具体的数据来源，有较好的可解释性和可追踪性。|黑盒子，可解释性相对较低。|
|计算资源|需要额外的资源来支持检索机制和数据库的维护。|依赖高质量的训练数据集和微调目标，对计算资源的要求较高。|
|推理延迟|增加了检索步骤的耗时|单纯 LLM 生成的耗时|
|降低幻觉|通过检索到的真实信息生成回答，降低了产生幻觉的概率。|模型学习特定领域的数据有助于减少幻觉，但面对未见过的输入时仍可能出现幻觉。|
|伦理隐私|检索和使用外部数据可能引发伦理和隐私方面的问题。|训练数据中的敏感信息需要妥善处理，以防泄露。|

### [1.2.3 RAG 的成功案例](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_123-rag-%e7%9a%84%e6%88%90%e5%8a%9f%e6%a1%88%e4%be%8b)

RAG 已经在多个领域取得了成功，包括问答系统、对话系统、文档摘要、文档生成等。

> 本章我们对 RAG 有了简单了解，在下一章我们将介绍一个常用的 RAG 开发框架 LangChain。

【**参考内容**】：

1. [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)
2. [面向大语言模型的检索增强生成技术：综述 [译]](https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey)

## [1.3 LangChain](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_13-langchain)

ChatGPT 的巨大成功激发了越来越多的开发者兴趣，他们希望利用 OpenAI 提供的 API 或者私有化模型，来开发基于大型语言模型的应用程序。尽管大型语言模型的调用相对简单，但要创建完整的应用程序，仍然需要大量的定制开发工作，包括 API 集成、互动逻辑、数据存储等等。

为了解决这个问题，从 2022 年开始，许多机构和个人相继推出了多个开源项目，旨在**帮助开发者们快速构建基于大型语言模型的端到端应用程序或工作流程**。其中一个备受关注的项目就是 LangChain 框架。

**LangChain 框架是一个开源工具，充分利用了大型语言模型的强大能力，以便开发各种下游应用。它的目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程**。具体来说，LangChain 框架可以实现数据感知和环境互动，也就是说，它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。

利用 LangChain 框架，我们可以轻松地构建如下所示的 RAG 应用（[图片来源](https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/langchain+chatglm.png)）。在下图中，`每个椭圆形代表了 LangChain 的一个模块`，例如数据收集模块或预处理模块。`每个矩形代表了一个数据状态`，例如原始数据或预处理后的数据。箭头表示数据流的方向，从一个模块流向另一个模块。在每一步中，LangChain 都可以提供对应的解决方案，帮助我们处理各种任务。

### [1.3.1 LangChain 的核心组件](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_131-langchain-%e7%9a%84%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6)

LangChian 作为一个大语言模型开发框架，可以将 LLM 模型（对话模型、embedding 模型等）、向量数据库、交互层 Prompt、外部知识、外部代理工具整合到一起，进而可以自由构建 LLM 应用。 LangChain 主要由以下 6 个核心组件组成:

- **模型输入/输出（Model I/O）**：与语言模型交互的接口
- **数据连接（Data connection）**：与特定应用程序的数据进行交互的接口
- **链（Chains）**：将组件组合实现端到端应用。比如后续我们会将搭建`检索问答链`来完成检索问答。
- **记忆（Memory）**：用于链的多次运行之间持久化应用程序状态；
- **代理（Agents）**：扩展模型的推理能力。用于复杂的应用的调用序列；
- **回调（Callbacks）**：扩展模型的推理能力。用于复杂的应用的调用序列；

在开发过程中，我们可以根据自身需求灵活地进行组合。

### [1.3.2 LangChain 的稳定版本](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_132-langchain-%e7%9a%84%e7%a8%b3%e5%ae%9a%e7%89%88%e6%9c%ac)

在 LLM 技术领域的迅猛发展浪潮中，LangChain 作为一个不断进化的创新平台，持续推动着技术边界的拓展。`2024 年 1 月 9 日`，LangChain 正式发布了其稳定版本 **v0.1.0**，这一里程碑式的更新，为开发者带来了全面而强大的功能支持。其涵盖了模型的输入与输出处理、数据连接、链式操作、记忆机制、代理服务以及回调处理等关键组件，为 LLM 应用的开发和部署提供了坚实的基础。 同时，LangChain 的持续优化和功能迭代，未来将带来更多创新特性和性能提升。

- **兼容性与支持**：LangChain v0.1.0 版本兼顾了对 `Python 和 JavaScript` 的支持，同时保持了向后兼容性，确保开发者能够在升级过程中无缝过渡，享受到更加安全稳定的开发体验。
    
- **架构改进**：通过将核心组件 langchain-core 与合作伙伴包进行有效分离，LangChain 的架构设计变得更加条理清晰和稳固，为未来的系统化扩展和安全性提升奠定了坚实基础。
    
- **可观察性**：LangChain 通过与 LangSmith 的深度集成，提供了业界领先的调试和观测功能。这使得开发者能够对 LLM 应用中的每一步操作及其输入输出有一个清晰的认识，极大地简化了调试和问题排查的流程。
    
- **广泛的集成**：LangChain 拥有近 **700** 个集成，覆盖了从 LLM 到向量存储、工具和智能体（Agent）等多个技术领域，极大地降低了在各种技术栈上构建 LLM 应用的复杂度。
    
- **可组合性**：借助 `LangChain 表达式语言（LCEL）`，开发者可以轻松地构建和定制 chain，充分利用数据编排框架的优势，包括批量处理、并行化操作和备选方案等高级功能。
    
- **流式处理**：LangChain 对流式处理进行了深度优化，确保所有利用 LCEL 创建的 chain 均能支持流式处理，包括中间步骤的数据流传输，从而为用户提供更加流畅的体验。
    
- **输出解析**：LangChain 提供了一系列强大的输出解析工具，确保 LLM 能够以结构化的格式返回信息，这对于 LLM 执行具体行动计划至关重要。
    
- **检索能力**：LangChain 引入了先进的检索技术，适用于生产环境，包括文本分割、检索机制和索引管道等，使得开发者能够轻松地将私有数据与 LLM 的能力相结合。
    
- **工具使用与智能体**：LangChain 提供了丰富的智能体和工具集合，并提供了定义工具的简便方法，支持智能体工作负载，包括让 LLM 调用函数或工具，以及如何高效地进行多次调用和推理，极大地提升了开发效率和应用性能。
    

### [1.3.3 LangChain 的生态](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_133-langchain-%e7%9a%84%e7%94%9f%e6%80%81)

- **LangChain Community**: 专注于第三方集成，极大地丰富了 LangChain 的生态系统，使得开发者可以更容易地构建复杂和强大的应用程序，同时也促进了社区的合作和共享。
    
- **LangChain Core**: LangChain 框架的核心库、核心组件，提供了基础抽象和 LangChain 表达式语言（LCEL），提供基础架构和工具，用于构建、运行和与 LLM 交互的应用程序，为 LangChain 应用程序的开发提供了坚实的基础。我们后续会用到的处理文档、格式化 prompt、输出解析等都来自这个库。
    
- **LangChain CLI**: 命令行工具，使开发者能够通过终端与 LangChain 框架交互，执行项目初始化、测试、部署等任务。提高开发效率，让开发者能够通过简单的命令来管理整个应用程序的生命周期。
    
- **LangServe**: 部署服务，用于将 LangChain 应用程序部署到云端，提供可扩展、高可用的托管解决方案，并带有监控和日志功能。简化部署流程，让开发者可以专注于应用程序的开发，而不必担心底层的基础设施和运维工作。
    
- **LangSmith**: 开发者平台，专注于 LangChain 应用程序的开发、调试和测试，提供可视化界面和性能分析工具，旨在帮助开发者提高应用程序的质量，确保它们在部署前达到预期的性能和稳定性标准。
    

---

> 本章我们简单介绍了开发框架 LangChain，下一章我们将介绍开发 LLM 应用的整体流程。

## [1.4 大模型开发](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_14-%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%bc%80%e5%8f%91)

我们将开发**以大语言模型为功能核心、通过大语言模型的强大理解能力和生成能力、结合特殊的数据或业务逻辑来提供独特功能的应用**称为**大模型开发**。开发大模型相关应用，其技术核心点虽然在大语言模型上，但一般通过调用 API 或开源模型来实现核心的理解与生成，通过 Prompt Enginnering 来实现大语言模型的控制，因此，虽然大模型是深度学习领域的集大成之作，大模型开发却更多是一个**工程问题**。

在大模型开发中，我们一般不会去大幅度改动模型，而是**将大模型作为一个调用工具，通过 Prompt Engineering、数据工程、业务逻辑分解等手段来充分发挥大模型能力，适配应用任务**，而不会将精力聚焦在优化模型本身上。因此，作为大模型开发的初学者，我们并不需要深研大模型内部原理，而更需要掌握使用大模型的实践技巧。

同时，以调用、发挥大模型为核心的大模型开发与传统的 AI 开发在**整体思路**上有着较大的不同。大语言模型的两个核心能力：`指令遵循`与`文本生成`提供了复杂业务逻辑的简单平替方案。

- `传统的 AI 开发`：首先需要将非常复杂的业务逻辑依次拆解，对于每一个子业务构造训练数据与验证数据，对于每一个子业务训练优化模型，最后形成完整的模型链路来解决整个业务逻辑。
- `大模型开发`：用 Prompt Engineering 来替代子模型的训练调优，通过 Prompt 链路组合来实现业务逻辑，用一个通用大模型 + 若干业务 Prompt 来解决任务，从而将传统的模型训练调优转变成了更简单、轻松、低成本的 Prompt 设计调优。

同时，在**评估思路**上，大模型开发与传统 AI 开发也有质的差异。

- `传统 AI 开发`：需要首先构造训练集、测试集、验证集，通过在训练集上训练模型、在测试集上调优模型、在验证集上最终验证模型效果来实现性能的评估。
- `大模型开发`：流程更为灵活和敏捷。从实际业务需求出发构造小批量验证集，设计合理 Prompt 来满足验证集效果。然后，将不断从业务逻辑中收集当下 Prompt 的 Bad Case，并将 Bad Case 加入到验证集中，针对性优化 Prompt，最后实现较好的泛化效果。
在本章中，我们将简述大模型开发的一般流程，并结合项目实际需求，逐步分析完成项目开发的工作和步骤。

### [1.4.1 大模型开发的一般流程](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_141-%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%bc%80%e5%8f%91%e7%9a%84%e4%b8%80%e8%88%ac%e6%b5%81%e7%a8%8b)

结合上述分析，我们一般可以将大模型开发分解为以下几个流程：
1. **确定目标**。在进行开发前，我们首先需要确定开发的目标，即要开发的应用的应用场景、目标人群、核心价值。对于个体开发者或小型开发团队而言，一般应先设定最小化目标，从构建一个 MVP（最小可行性产品）开始，逐步进行完善和优化。
    
2. **设计功能**。在确定开发目标后，需要设计本应用所要提供的功能，以及每一个功能的大体实现逻辑。虽然我们通过使用大模型来简化了业务逻辑的拆解，但是越清晰、深入的业务逻辑理解往往也能带来更好的 Prompt 效果。同样，对于个体开发者或小型开发团队来说，首先要确定应用的核心功能，然后延展设计核心功能的上下游功能；例如，我们想打造一款个人知识库助手，那么核心功能就是结合个人知识库内容进行问题的回答，那么其上游功能的用户上传知识库、下游功能的用户手动纠正模型回答就是我们也必须要设计实现的子功能。
    
3. **搭建整体架构**。目前，绝大部分大模型应用都是采用的特定数据库 + Prompt + 通用大模型的架构。我们需要针对我们所设计的功能，搭建项目的整体架构，实现从用户输入到应用输出的全流程贯通。一般来说，我们推荐基于 LangChain 框架进行开发。LangChain 提供了 Chain、Tool 等架构的实现，我们可以基于 LangChain 进行个性化定制，实现从用户输入到数据库再到大模型最后输出的整体架构连接。
    
4. **搭建数据库**。个性化大模型应用需要有个性化数据库进行支撑。由于大模型应用需要进行向量语义检索，一般使用诸如 Chroma 的向量数据库。在该步骤中，我们需要收集数据并进行预处理，再向量化存储到数据库中。数据预处理一般包括从多种格式向纯文本的转化，例如 PDF、MarkDown、HTML、音视频等，以及对错误数据、异常数据、脏数据进行清洗。完成预处理后，需要进行切片、向量化构建出个性化数据库。
    
5. **Prompt Engineering**。优质的 Prompt 对大模型能力具有极大影响，我们需要逐步迭代构建优质的 Prompt Engineering 来提升应用性能。在该步中，我们首先应该明确 Prompt 设计的一般原则及技巧，构建出一个来源于实际业务的小型验证集，基于小型验证集设计满足基本要求、具备基本能力的 Prompt。
    
6. **验证迭代**。验证迭代在大模型开发中是极其重要的一步，一般指通过不断发现 Bad Case 并针对性改进 Prompt Engineering 来提升系统效果、应对边界情况。在完成上一步的初始化 Prompt 设计后，我们应该进行实际业务测试，探讨边界情况，找到 Bad Case，并针对性分析 Prompt 存在的问题，从而不断迭代优化，直到达到一个较为稳定、可以基本实现目标的 Prompt 版本。
    
7. **前后端搭建**。完成 Prompt Engineering 及其迭代优化之后，我们就完成了应用的核心功能，可以充分发挥大语言模型的强大能力。接下来我们需要搭建前后端，设计产品页面，让我们的应用能够上线成为产品。前后端开发是非常经典且成熟的领域，此处就不再赘述，我们采用 Gradio 和 Streamlit，可以帮助个体开发者迅速搭建可视化页面实现 Demo 上线。
    
8. **体验优化**。在完成前后端搭建之后，应用就可以上线体验了。接下来就需要进行长期的用户体验跟踪，记录 Bad Case 与用户负反馈，再针对性进行优化即可。
### [1.4.2 搭建 LLM 项目的流程简析（以知识库助手为例）](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_142-%e6%90%ad%e5%bb%ba-llm-%e9%a1%b9%e7%9b%ae%e7%9a%84%e6%b5%81%e7%a8%8b%e7%ae%80%e6%9e%90%ef%bc%88%e4%bb%a5%e7%9f%a5%e8%af%86%e5%ba%93%e5%8a%a9%e6%89%8b%e4%b8%ba%e4%be%8b%ef%bc%89)

以下我们将结合本实践项目与上文的整体流程介绍，简要分析[知识库助手项目](https://github.com/logan-zou/Chat_with_Datawhale_langchain)开发流程：

#### [1.4.2.1 项目规划与需求分析](https://datawhalechina.github.io/llm-universe/#/C1/C1?id=_1421-%e9%a1%b9%e7%9b%ae%e8%a7%84%e5%88%92%e4%b8%8e%e9%9c%80%e6%b1%82%e5%88%86%e6%9e%90)

1. **项目目标**：基于个人知识库的问答助手
    
2. **核心功能**
    
    1. 将爬取并总结的 MarkDown 文件及用户上传文档向量化，并创建知识库；
    2. 选择知识库，检索用户提问的知识片段；
    3. 提供知识片段与提问，获取大模型回答；
    4. 流式回复；
    5. 历史对话记录
3. **确定技术架构和工具**
    
    1. **框架**：LangChain
    2. **Embedding 模型**：GPT、智谱、[M3E](https://huggingface.co/moka-ai/m3e-base)
    3. **数据库**：Chroma
    4. **大模型**：GPT、讯飞星火、文心一言、GLM 等
    5. **前端**：Gradio 和 Streamlit
4. **数据准备与向量知识库构建**
    

本项目实现原理如下图所示（[图片来源](https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/langchain+chatglm.png)）：加载本地文档 -> 读取文本 -> 文本分割 -> 文本向量化 -> question 向量化 -> 在文本向量中匹配出与问句向量最相似的 top k 个 -> 匹配出的文本作为上下文和问题一起添加到 Prompt 中 -> 提交给 LLM 生成回答。

1. 收集和整理用户提供的文档
    
    用户常用文档格式有 PDF、TXT、MD 等，首先，我们可以使用 LangChain 的文档加载器模块方便地加载用户提供的文档，或者使用一些成熟的 Python 包进行读取。
    
    由于目前大模型使用 token 的限制，我们需要对读取的文本进行切分，将较长的文本切分为较小的文本，这时一段文本就是一个单位的知识。
    
2. 将文档词向量化
    
    使用`文本嵌入(Embeddings)技术`对分割后的文档进行向量化，使语义相似的文本片段具有接近的向量表示。然后，存入向量数据库，完成 `索引(index)` 的创建。
    
    利用向量数据库对各文档片段进行索引，可以实现快速检索。
    
3. 将向量化后的文档导入 Chroma 知识库，建立知识库索引
    
    Langchain 集成了超过 30 个不同的向量数据库。Chroma 数据库轻量级且数据存储在内存中，这使得它非常容易启动和开始使用。
    
    将用户知识库内容经过 Embedding 存入向量数据库，然后用户每一次提问也会经过 Embedding，利用向量相关性算法（例如余弦算法）找到最匹配的几个知识库片段，将这些知识库片段作为上下文，与用户问题一起作为 Prompt 提交给 LLM 回答。
    
4. 大模型集成与 API 连接
    
    1. 集成 GPT、星火、文心、GLM 等大模型，配置 API 连接。
    2. 编写代码，实现与大模型 API 的交互，以便获取问题回答。
5. 核心功能实现
    
    1. 构建 Prompt Engineering，实现大模型回答功能，根据用户提问和知识库内容生成回答。
    2. 实现流式回复，允许用户进行多轮对话。
    3. 添加历史对话记录功能，保存用户与助手的交互历史。
6. 核心功能迭代优化
    
    1. 进行验证评估，收集 Bad Case。
    2. 根据 Bad Case 迭代优化核心功能实现。
7. 前端与用户交互界面开发
    
    1. 使用 Gradio 和 Streamlit 搭建前端界面。
    2. 实现用户上传文档、创建知识库的功能。
    3. 设计用户界面，包括问题输入、知识库选择、历史记录展示等。
8. 部署测试与上线
    
    1. 部署问答助手到服务器或云平台，确保可在互联网上访问。
    2. 进行生产环境测试，确保系统稳定。
    3. 上线并向用户发布。
9. 维护与持续改进
    
    1. 监测系统性能和用户反馈，及时处理问题。
    2. 定期更新知识库，添加新的文档和信息。
    3. 收集用户需求，进行系统改进和功能扩展。

整个流程将确保项目从规划、开发、测试到上线和维护都能够顺利进行，为用户提供高质量的基于个人知识库的问答助手。

---

> 现在我们已经对大模型开发的一般流程有了初步了解，接下来我们将针对整个开发环境进行介绍，确保大家可以顺利的进行项目开发。
> 
> - 如果大家是老手可以直接跳过本章后续内容，直接进入第二部分学习。
> - 后续两章主要针对没有合适开发环境的同学介绍两种开发环境的搭建，大家可以按需阅读。
>     - 第五章主要介绍`阿里云服务器的基本使用`、`通过 SSH 远程连接服务器`、`jupyter notebook 的使用`。
>     - 第六章节主要介绍了`GitHub CodeSpace`的使用，以及如何在`GitHub CodeSpace`中搭建开发环境。（首先确定是否具有可以流畅访问 GitHub 的网络环境，否则建议使用阿里云）
> - 如果大家手中已经有合适的开发机，可以直接跳到`7.环境配置`章节，开始配置开发环境。